{"cells":[{"cell_type":"code","execution_count":1,"id":"b7dc8574-e69e-41ee-a991-63d88e26d5a5","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["# Base path to csv files\n","base_path = \"../raw-files/Human Resources/\"\n","\n","# List of file names\n","file_names = [\n","    'HumanResources Department.csv',\n","    'HumanResources Employee.csv',\n","    'HumanResources EmployeeDepartmentHistory.csv',\n","    'HumanResources EmployeePayHistory.csv',\n","    'HumanResources JobCandidate.csv',\n","    'HumanResources Shift.csv'\n","]"]},{"cell_type":"code","execution_count":2,"id":"1e49750a-9eb6-474b-b316-43906794ab25","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"name":"stderr","output_type":"stream","text":["24/06/24 13:33:49 WARN Utils: Your hostname, Joshs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.100.2 instead (on interface en0)\n","24/06/24 13:33:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/06/24 13:33:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","                                                                                \r"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName('HR').getOrCreate()\n","\n","# Read each file into a DataFrame\n","dataframes = {}\n","for file_name in file_names:\n","    df_name = file_name.replace(' ', '_').replace('.csv', '').lower()\n","    dataframes[df_name] = spark.read.format('csv').option('header', 'true').load(f'{base_path}/{file_name}')\n","\n","# Accessing individual DataFrames\n","department_df = dataframes['humanresources_department']\n","employee_df = dataframes['humanresources_employee']\n","employeeDepartmentHistory_df = dataframes['humanresources_employeedepartmenthistory']\n","employeePayHistory_df = dataframes['humanresources_employeepayhistory']\n","jobCandidate_df = dataframes['humanresources_jobcandidate']\n","shift_df = dataframes['humanresources_shift']"]},{"cell_type":"code","execution_count":3,"id":"32b158d9-f9e4-4933-a299-63416b9f968d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def rename_columns(df, rename_mappings):\n","    for old_name, new_name in rename_mappings.items():\n","        df = df.withColumnRenamed(old_name,new_name)\n","    return df\n","\n","# Define the rename mappings for each DataFrame\n","rename_mappings = {\n","    'department_df': {'Name':'DepartmentName', 'ModifiedDate': 'DepartmentModifiedDate'},\n","    'employee_df': {'ModifiedDate': 'EmployeeModifiedDate'},\n","    'employeeDepartmentHistory_df': {'ModifiedDate': 'EmployeeDepartmentHistoryModifiedDate'},\n","    'employeePayHistory_df': {'ModifiedDate': 'EmployeePayHistoryModifiedDate'},\n","    'jobCandidate_df': {'ModifiedDate': 'JobCandidateModifiedDate'},\n","    'shift_df': {'Name':'ShiftName', 'ModifiedDate': 'ShiftModifiedDate'}\n","}\n","\n","department_df = rename_columns(department_df, rename_mappings['department_df'])\n","employee_df = rename_columns(employee_df, rename_mappings['employee_df'])\n","employeeDepartmentHistory_df = rename_columns(employeeDepartmentHistory_df, rename_mappings['employeeDepartmentHistory_df'])\n","employeePayHistory_df = rename_columns(employeePayHistory_df, rename_mappings['employeePayHistory_df'])\n","jobCandidate_df = rename_columns(jobCandidate_df, rename_mappings['jobCandidate_df'])\n","shift_df = rename_columns(shift_df, rename_mappings['shift_df'])"]},{"cell_type":"code","execution_count":4,"id":"e9aebc30-6f11-4df2-aba9-3c79cb29d1aa","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Join the employeeDepartmentHistory_df and department_df and shift_df\n","human_resources_details_df = employeeDepartmentHistory_df.join(department_df, \"DepartmentID\", \"left\")\n","human_resources_details_df = human_resources_details_df.join(shift_df,\"ShiftID\",\"left\")\n","\n","# Join other DataFrames\n","human_resources_details_df = human_resources_details_df.join(employee_df,\"BusinessEntityID\",\"left\")\n","human_resources_details_df = human_resources_details_df.join(jobCandidate_df, \"BusinessEntityID\",\"left\")\n","human_resources_details_df = human_resources_details_df.join(employeePayHistory_df,\"BusinessEntityID\",\"left\")\n"]},{"cell_type":"code","execution_count":6,"id":"c44a464e","metadata":{},"outputs":[],"source":["# save to csv file\n","output_path = \"../denormalized-files/human_resources.csv\"\n","\n","# Convert Spark DataFrame to pandas DataFrame\n","human_resources_details_pd_df = human_resources_details_df.toPandas()\n","\n","# Save to CSV using pandas, ensuring it's a single file\n","human_resources_details_pd_df.to_csv(output_path, index=False, header=True)"]},{"cell_type":"code","execution_count":null,"id":"adfea300-717a-4a4b-ad79-452b474d284d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":[]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"f19eeb81-b3fc-4d06-b0c4-5503d6b2ba1e","default_lakehouse_name":"AdventureWorks_lkh","default_lakehouse_workspace_id":"309dab8b-b180-45cd-a4ae-54edb25f342c","known_lakehouses":[{"id":"f19eeb81-b3fc-4d06-b0c4-5503d6b2ba1e"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
