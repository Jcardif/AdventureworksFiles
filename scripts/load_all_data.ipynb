{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"cycles\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path to csv files\n",
    "base_path = \"../raw-files/Person Data/\"\n",
    "\n",
    "# only keep the columns we need for the person data\n",
    "required_cols = [\n",
    "    \"BusinessEntityID\",\n",
    "    \"Title\",\n",
    "    \"FirstName\",\n",
    "    \"MiddleName\",\n",
    "    \"LastName\",\n",
    "    \"NameStyle\",\n",
    "    \"Demographics\",\n",
    "    \"Suffix\",\n",
    "    \"EmailAddress\",\n",
    "    \"AddressLine1\",\n",
    "    \"AddressLine2\",\n",
    "    \"PhoneNumber\",\n",
    "]\n",
    "\n",
    "# Read required files\n",
    "person_df = spark.read.csv(base_path + \"Person Person.csv\", header=True)\n",
    "emailAddress_df = spark.read.csv(base_path + \"Person EmailAddress.csv\", header=True)\n",
    "businessEntity_df = spark.read.csv(base_path + \"Person BusinessEntity.csv\", header=True)\n",
    "businessEntityAddress_df = spark.read.csv(base_path + \"Person BusinessEntityAddress.csv\", header=True)\n",
    "address_df = spark.read.csv(base_path + \"Person Address.csv\", header=True)\n",
    "personPhone_df = spark.read.csv(base_path + \"Person PersonPhone.csv\", header=True)\n",
    "\n",
    "# todo columns to add: customerkey, geographykey, birthdate,mariatalstatus, gender, yearlyincome, totalchildern, incomegroup, educattion, numberofchildrenatHome, homeownerflag, numberofcarsowned, age, region\n",
    "\n",
    "person_details_df = (\n",
    "    person_df.join(emailAddress_df, \"BusinessEntityID\", \"left\")\n",
    "    .join(businessEntity_df, \"BusinessEntityID\", \"left\")\n",
    "    .join(businessEntityAddress_df, \"BusinessEntityID\", \"left\")\n",
    "    .join(address_df, \"AddressID\", \"left\")\n",
    "    .join(personPhone_df, \"BusinessEntityID\", \"left\")\n",
    ")\n",
    "\n",
    "\n",
    "person_details_df = person_details_df.select(required_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path to csv files\n",
    "base_path = \"../raw-files/Sales Data/\"\n",
    "\n",
    "# only keep the columns we need for the sales data\n",
    "required_cols = [\n",
    "    \"CustomerKey\",\n",
    "    \"BusinessEntityID\",\n",
    "    \"Region\",\n",
    "    \"SalesOrderNumber\",\n",
    "    \"ProductID\",\n",
    "    \"OrderQty\",\n",
    "    \"UnitPrice\",\n",
    "]\n",
    "\n",
    "# Read required files\n",
    "customer_df = spark.read.csv(base_path + \"Sales Customer.csv\", header=True)\n",
    "salesOrderHeader_df = spark.read.csv(base_path + \"Sales SalesOrderHeader.csv\", header=True)\n",
    "salesOrderDetail_df = spark.read.csv(base_path + \"Sales SalesOrderDetail.csv\", header=True)\n",
    "salesTerritory_df = spark.read.csv(base_path + \"Sales SalesTerritory.csv\", header=True)\n",
    "\n",
    "sales_details_df = (\n",
    "    customer_df.join(salesOrderHeader_df, \"CustomerID\", \"left\")\n",
    "    .join(salesOrderDetail_df, \"SalesOrderID\", \"left\")\n",
    "    .join(salesTerritory_df, \"TerritoryID\", \"left\")\n",
    ")\n",
    "\n",
    "# rename columns : CustomerID -> CustomerKey, Group -> Region\n",
    "sales_details_df = (\n",
    "    sales_details_df.withColumnRenamed(\"CustomerID\", \"CustomerKey\")\n",
    "    .withColumnRenamed(\"Group\", \"Region\")\n",
    "    .withColumnRenamed(\"PersonID\", \"BusinessEntityID\")\n",
    ")\n",
    "\n",
    "sales_details_df = sales_details_df.select(required_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# Base path to csv files\n",
    "base_path = \"../raw-files/Production Data/\"\n",
    "\n",
    "# only keep the columns we need for the production data\n",
    "required_cols = [\"ProductID\", \"ProductCategoryName\", \"Model\"]\n",
    "\n",
    "# Read required files\n",
    "product_df = spark.read.csv(base_path + \"Production Product.csv\", header=True)\n",
    "productSubcategory_df = spark.read.csv(base_path + \"Production ProductSubcategory.csv\", header=True)\n",
    "productCategory_df = spark.read.csv(base_path + \"Production ProductCategory.csv\", header=True)\n",
    "productModel_df = spark.read.csv(base_path + \"Production ProductModel.csv\", header=True)\n",
    "\n",
    "\n",
    "product_details_df = product_df.join(productSubcategory_df, \"ProductSubCategoryID\", \"left\")\n",
    "\n",
    "# rename column to SubCategoryName\n",
    "product_details_df = product_details_df.withColumnRenamed(\"Name\", \"SubCategoryName\")\n",
    "\n",
    "product_details_df = product_details_df.join(productCategory_df, \"ProductCategoryID\", \"left\")\n",
    "\n",
    "# rename name column to ProductCategoryName\n",
    "product_details_df = product_details_df.withColumnRenamed(\"Name\", \"ProductCategoryName\")\n",
    "\n",
    "product_details_df = product_details_df.join(productModel_df, \"ProductModelID\", \"left\")\n",
    "\n",
    "# Add column Model which is a coalesce of Name and ProductCategoryName\n",
    "product_details_df = product_details_df.withColumn(\n",
    "    \"Model\",\n",
    "    coalesce(product_details_df[\"Name\"], product_details_df[\"ProductCategoryName\"]),\n",
    ")\n",
    "\n",
    "product_details_df = product_details_df.select(required_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/25 00:14:37 WARN TransportChannelHandler: Exception in connection from /192.168.100.95:63409\n",
      "java.io.IOException: Operation timed out\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:378)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# save files to csv\n",
    "person_output_path = \"../denormalized-files/person.csv\"\n",
    "sales_output_path =\"../denormalized-files/sales.csv\"\n",
    "products_output_path = \"../denormalized-files/products.csv\"\n",
    "\n",
    "# Convert Spark DataFrame to pandas DataFrame and save\n",
    "person_details_df.toPandas().to_csv(person_output_path, index=False, header=True)\n",
    "sales_details_df.toPandas().to_csv(sales_output_path, index=False, header=True)\n",
    "product_details_df.toPandas().to_csv(products_output_path, index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
