{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"cycles\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path to csv files\n",
    "base_path = \"../raw-files/Person Data/\"\n",
    "\n",
    "# only keep the columns we need for the person data\n",
    "required_cols = [\n",
    "    \"PersonID\",\n",
    "    \"Title\",\n",
    "    \"FirstName\",\n",
    "    \"MiddleName\",\n",
    "    \"LastName\",\n",
    "    \"NameStyle\",\n",
    "    \"Demographics\",\n",
    "    \"Suffix\",\n",
    "    \"EmailAddress\",\n",
    "    \"AddressLine1\",\n",
    "    \"AddressLine2\",\n",
    "    \"PhoneNumber\",\n",
    "]\n",
    "\n",
    "# Read required files\n",
    "person_df = spark.read.csv(base_path + \"Person Person.csv\", header=True)\n",
    "emailAddress_df = spark.read.csv(base_path + \"Person EmailAddress.csv\", header=True)\n",
    "businessEntity_df = spark.read.csv(base_path + \"Person BusinessEntity.csv\", header=True)\n",
    "businessEntityAddress_df = spark.read.csv(base_path + \"Person BusinessEntityAddress.csv\", header=True)\n",
    "address_df = spark.read.csv(base_path + \"Person Address.csv\", header=True)\n",
    "personPhone_df = spark.read.csv(base_path + \"Person PersonPhone.csv\", header=True)\n",
    "\n",
    "person_details_df = (\n",
    "    person_df.join(emailAddress_df, \"BusinessEntityID\", \"left\")\n",
    "    .join(businessEntity_df, \"BusinessEntityID\", \"left\")\n",
    "    .join(businessEntityAddress_df, \"BusinessEntityID\", \"left\")\n",
    "    .join(address_df, \"AddressID\", \"left\")\n",
    "    .join(personPhone_df, \"BusinessEntityID\", \"left\")\n",
    ")\n",
    "\n",
    "person_details_df = person_details_df.withColumnRenamed(\"BusinessEntityID\", \"PersonID\")\n",
    "\n",
    "person_details_df = person_details_df.select(required_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path to csv files\n",
    "base_path = \"../raw-files/Sales Data/\"\n",
    "\n",
    "# only keep the columns we need for the sales data\n",
    "required_cols = [\n",
    "    \"CustomerKey\",\n",
    "    \"SalesPersonID\",\n",
    "    \"PersonID\",\n",
    "    \"Region\",\n",
    "    \"SalesOrderNumber\",\n",
    "    \"ProductID\",\n",
    "    \"OrderQty\",\n",
    "    \"OrderDate\",\n",
    "    \"ShipDate\",\n",
    "    \"UnitPrice\",\n",
    "    \"UnitPriceDiscount\",\n",
    "]\n",
    "\n",
    "# Read required files\n",
    "customer_df = spark.read.csv(base_path + \"Sales Customer.csv\", header=True)\n",
    "salesOrderHeader_df = spark.read.csv(base_path + \"Sales SalesOrderHeader.csv\", header=True)\n",
    "salesOrderDetail_df = spark.read.csv(base_path + \"Sales SalesOrderDetail.csv\", header=True)\n",
    "salesTerritory_df = spark.read.csv(base_path + \"Sales SalesTerritory.csv\", header=True)\n",
    "\n",
    "sales_details_df = (\n",
    "    customer_df.join(salesOrderHeader_df, \"CustomerID\", \"left\")\n",
    "    .join(salesOrderDetail_df, \"SalesOrderID\", \"left\")\n",
    "    .join(salesTerritory_df, \"TerritoryID\", \"left\")\n",
    ")\n",
    "\n",
    "# rename columns : Group -> Region CustomerID -> CustomerKey\n",
    "sales_details_df = (sales_details_df\n",
    "                    .withColumnRenamed(\"Group\", \"Region\")\n",
    "                    .withColumnRenamed(\"CustomerID\", \"CustomerKey\")\n",
    ")\n",
    "\n",
    "sales_details_df = sales_details_df.select(required_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# Base path to csv files\n",
    "base_path = \"../raw-files/Production Data/\"\n",
    "\n",
    "# only keep the columns we need for the production data\n",
    "required_cols = [\"ProductID\", \"ProductCategoryName\", \"Model\"]\n",
    "\n",
    "# Read required files\n",
    "product_df = spark.read.csv(base_path + \"Production Product.csv\", header=True)\n",
    "productSubcategory_df = spark.read.csv(base_path + \"Production ProductSubcategory.csv\", header=True)\n",
    "productCategory_df = spark.read.csv(base_path + \"Production ProductCategory.csv\", header=True)\n",
    "productModel_df = spark.read.csv(base_path + \"Production ProductModel.csv\", header=True)\n",
    "\n",
    "\n",
    "product_details_df = product_df.join(productSubcategory_df, \"ProductSubCategoryID\", \"left\")\n",
    "\n",
    "# rename column to SubCategoryName\n",
    "product_details_df = product_details_df.withColumnRenamed(\"Name\", \"SubCategoryName\")\n",
    "\n",
    "product_details_df = product_details_df.join(productCategory_df, \"ProductCategoryID\", \"left\")\n",
    "\n",
    "# rename name column to ProductCategoryName\n",
    "product_details_df = product_details_df.withColumnRenamed(\"Name\", \"ProductCategoryName\")\n",
    "\n",
    "product_details_df = product_details_df.join(productModel_df, \"ProductModelID\", \"left\")\n",
    "\n",
    "# Add column Model which is a coalesce of Name and ProductCategoryName\n",
    "product_details_df = product_details_df.withColumn(\n",
    "    \"Model\",\n",
    "    coalesce(product_details_df[\"Name\"], product_details_df[\"ProductCategoryName\"]),\n",
    ")\n",
    "\n",
    "product_details_df = product_details_df.select(required_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files to csv\n",
    "person_output_path = \"../workshop-files/person.csv\"\n",
    "sales_output_path = \"../workshop-files/sales.csv\"\n",
    "products_output_path = \"../workshop-files/products.csv\"\n",
    "\n",
    "# Convert Spark DataFrame to pandas DataFrame and save\n",
    "person_details_df.toPandas().to_csv(person_output_path, index=False, header=True)\n",
    "sales_details_df.toPandas().to_csv(sales_output_path, index=False, header=True)\n",
    "product_details_df.toPandas().to_csv(products_output_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join sales and person data\n",
    "customer_details = sales_details_df.join(person_details_df, \"PersonID\", \"inner\")\n",
    "\n",
    "cust_prod_details = customer_details.join(product_details_df, \"ProductID\", \"inner\")\n",
    "\n",
    "# filter where model or product category name is bikes\n",
    "cust_prod_details = cust_prod_details.filter(\n",
    "    (cust_prod_details[\"Model\"] == \"Bikes\") | (cust_prod_details[\"ProductCategoryName\"] == \"Bikes\")\n",
    ").select(\"PersonID\", \"Region\", \"ProductCategoryName\", \"Model\", \"Demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+--------------+--------------------+\n",
      "|PersonID|       Region|ProductCategoryName|         Model|        Demographics|\n",
      "+--------+-------------+-------------------+--------------+--------------------+\n",
      "|    4937|North America|              Bikes|  Mountain-200|\"<IndividualSurve...|\n",
      "|    4937|North America|              Bikes|  Mountain-200|\"<IndividualSurve...|\n",
      "|    6731|       Europe|              Bikes|    Road-550-W|\"<IndividualSurve...|\n",
      "|    7273|      Pacific|              Bikes|  Mountain-200|\"<IndividualSurve...|\n",
      "|    7273|      Pacific|              Bikes|  Touring-1000|\"<IndividualSurve...|\n",
      "|    7273|      Pacific|              Bikes|  Mountain-100|\"<IndividualSurve...|\n",
      "|   11722|North America|              Bikes|      Road-650|\"<IndividualSurve...|\n",
      "|   11722|North America|              Bikes|  Mountain-200|\"<IndividualSurve...|\n",
      "|   12394|       Europe|              Bikes|  Mountain-200|\"<IndividualSurve...|\n",
      "|   12394|       Europe|              Bikes|  Touring-1000|\"<IndividualSurve...|\n",
      "|   12394|       Europe|              Bikes|      Road-150|\"<IndividualSurve...|\n",
      "|   14899|North America|              Bikes|      Road-750|\"<IndividualSurve...|\n",
      "|   17714|       Europe|              Bikes|  Touring-3000|\"<IndividualSurve...|\n",
      "|    5645|North America|              Bikes|      Road-750|\"<IndividualSurve...|\n",
      "|    9586|North America|              Bikes|  Mountain-200|\"<IndividualSurve...|\n",
      "|   11078|North America|              Bikes|      Road-250|\"<IndividualSurve...|\n",
      "|   11078|North America|              Bikes|  Mountain-200|\"<IndividualSurve...|\n",
      "|    2464|North America|              Bikes|      Road-750|\"<IndividualSurve...|\n",
      "|    3368|North America|              Bikes|Mountain-400-W|\"<IndividualSurve...|\n",
      "|    3441|       Europe|              Bikes|      Road-250|\"<IndividualSurve...|\n",
      "+--------+-------------+-------------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_prod_details.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
